<div align="center">
  <h1>ğŸ¨ MultiModalHugs</h1>
</div>

**MultimodalHugs** is a lightweight, modular framework built on top of [Hugging Face](https://huggingface.co/) for training, evaluating, and deploying **multimodal AI models** with minimal code.

It supports diverse input modalitiesâ€”including text, images, video, and pose sequencesâ€”and integrates seamlessly with the Hugging Face ecosystem (Trainer API, model hub, `evaluate`, etc.).

---

## Key Features

- âœ… **Minimal boilerplate**: Standardized TSV format for datasets and YAML-based configuration.
- ğŸ” **Reproducible pipelines**: Consistent setup for training, evaluation, and inference.
- ğŸ”Œ **Modular design**: Easily extend or swap models, processors, and modalities.
- ğŸ“¦ **Hugging Face native**: Built to work out-of-the-box with existing models and tools.
- **Examples Included**: Refer to the `examples/` directory for guided scripts, configurations, and best practices.
  
Whether you're working on sign language translation, image-to-text, or token-free language modeling, MultimodalHugs simplifies experimentation while keeping your codebase clean.

For more details, refer to the [documentation](docs/README.md).

---

## Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/GerrySant/multimodalhugs.git
   ```

2. **Navigate and install the package**:

   - **Standard installation**:
      ```bash
       cd multimodalhugs
       pip install .
      ```
   - **Developer installation**:
      ```bash
       cd multimodalhugs
       pip install -e .[dev]
      ```

## Usage

### ğŸš€ Getting Started

To set up, train, and evaluate a model, follow these steps:

![Steps Overview](docs/media/steps.png)

### 1. Dataset Preparation

For each partition (train, val, test), create a TSV file that captures essential sample details for consistency.

#### Metadata File Requirements

The `metadata.tsv` files for each partition must include the following fields:

- `signal`: The primary input to the model, either as raw text or a file path pointing to a multimodal resource (e.g., an image, pose sequence, or audio file).
- `signal_start`: Start timestamp (commonly in milliseconds) of the input segment. Can be left empty or `0` if not required by the setup.
- `signal_end`: End timestamp (commonly in milliseconds) of the input segment. Can be left empty or `0` if not required by the setup.
- `encoder_prompt`: An optional text field providing additional context to the input; this may include instructions (e.g., `Translate the pose to English`), modality tags (e.g., `__asl__` for American Sign Languge, ASL), or any text relevant to the task.
- `decoder_prompt`: An optional textual prompt used during decoding to guide the modelâ€™s output generation, corresponding to Hugging Faceâ€™s `decoder_input_ids`.
- `output`: The expected textual output corresponding to the input signal.


### 2. Setup Datasets, Model, and Processors

```bash
multimodalhugs-setup --modality {pose2text,signwriting2text,image2text,etc} --config_path $CONFIG_PATH --output_dir $OUTPUT_PATH
```

### 3. Train a Model

```bash
multimodalhugs-train --task <task_name> --config_path $CONFIG_PATH --output_dir $OUTPUT_PATH
```

### 4. Generate Outputs with a Trained Model

```bash
multimodalhugs-generate --task <task_name> \
      --metric_name $METRIC_NAME \
      --config_path $CONFIG_PATH \
      --model_name_or_path $CKPT_PATH \
      --processor_name_or_path $PROCESSOR_PATH \
      --dataset_dir $DATASET_PATH \
      --output_dir $GENERATION_OUTPUT_DIR
```


For more details, refer to the [CLI documentation](docs/general/CLI.md).

[Here](/examples/multimodal_translation/) you can find some sample end-to-end experimentation pipelines.

## Directory Overview

```yaml
multimodalhugs/
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ LICENSE                 # License information
â”œâ”€â”€ pyproject.toml          # Package dependencies and setup
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ .github/                # GitHub actions and workflows
â”‚   â””â”€â”€ workflows/
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ customization/      # Guides for custom extensions
â”‚   â”œâ”€â”€ data/               # Data configs and dataset docs
â”‚   â”œâ”€â”€ general/            # General framework documentation
â”‚   â”œâ”€â”€ media/              # Visual guides
â”‚   â””â”€â”€ models/             # Model documentation
â”œâ”€â”€ examples/               # Example scripts and configurations
â”‚   â””â”€â”€ multimodal_translation/
â”‚       â”œâ”€â”€ image2text_translation/
â”‚       â”œâ”€â”€ pose2text_translation/
â”‚       â””â”€â”€ signwriting2text_translation/
â”œâ”€â”€ multimodalhugs/         # Core framework
â”‚   â”œâ”€â”€ custom_datasets/    # Custom datasets
â”‚   â”œâ”€â”€ data/               # Data handling utilities
â”‚   â”œâ”€â”€ models/             # Model implementations
â”‚   â”œâ”€â”€ modules/            # Custom components (adapters, embeddings, etc.)
â”‚   â”œâ”€â”€ processors/         # Preprocessing modules
â”‚   â”œâ”€â”€ tasks/              # Task-specific logic (e.g., translation)
â”‚   â”œâ”€â”€ training_setup/     # Training pipeline setup
â”‚   â”œâ”€â”€ multimodalhugs_cli/ # Command-line interface for training/inference
â”‚   â””â”€â”€ utils/              # Helper functions
â”œâ”€â”€ scripts/                # Utility scripts (e.g., docs generation, metrics)
â””â”€â”€ tests/                  # Unit and integration tests

```

For a detailed breakdown of each directory, see [docs/README.md](docs/README.md).

## Contributing

All contributionsâ€”bug reports, feature requests, or pull requestsâ€”are welcome. Please see our [GitHub repository](https://github.com/GerrySant/multimodalhugs) to get involved.

## License

This project is licensed under the terms of the MIT License.

## Citing this Work

If you use MultimodalHugs in your research or applications, please cite:

```bibtex
@misc{sant2025multimodalhugs,
  title        = {MultimodalHugs: Enabling Sign Language Processing in Hugging Face},
  author       = {Sant, Gerard and Jiang, Zifan and Escolano, Carlos and Moryossef, Amit and MÃ¼ller, Mathias and Sennrich, Rico and Ebling, Sarah},
  year         = {2024},
  note         = {Manuscript submitted for publication},
  howpublished = {https://github.com/GerrySant/multimodalhugs},
}
```