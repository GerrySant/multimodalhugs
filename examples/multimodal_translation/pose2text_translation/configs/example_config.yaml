# Configuration file for training with the multimodalhugs framework.
# This file contains settings for the model, common experiment parameters,
# training hyperparameters, and dataset/data loading options.

model:
  # Model-specific settings:
  type: "multimodal_embedder" 
  vl_mapper_type: "linear"                                # Type of visual-language mapper (e.g., "linear" or "adapter").
  vl_mapper_layer_norm_before: true                       # Whether to apply Layer Normalization before the VL mapper.
  vl_mapper_dropout: 0.1                                  # Dropout probability for the VL mapper to prevent overfitting.
  backbone_name: "<pretrained-backbone-model>"            # Identifier for the pretrained backbone (e.g., "m2m100").
  pretrained_backbone: "<pretrained-backbone-weights>"    # Weights or checkpoint identifier for the pretrained backbone.
  feat_dim: 534                                           # Dimensionality of the features produced by the backbone or feature extractor.
  init_lang_abbr: avg                                     # Abbreviation of the language used to initialize new language embeddings. (avg means that the new embeddings will be inicialized from the avg of the pretrained embeddings.)

training:
  run_name: "how2sign_pose_2_text_model"           # The name or identifier of the model configuration.
  logging_dir: "/path/to/output/logs"              # Directory to store training logs.
  do_train: True                                   # Whether to run training.
  do_eval: True                                    # Whether to run evaluation on the validation set.
  predict_with_generate: true                      # Use generate to compute generative metrics.
  overwrite_output_dir: True                       # Do not overwrite the output directory if it exists.
  save_strategy: "steps"                           # Checkpoint save strategy during training.
  evaluation_strategy: "steps"                     # Evaluation strategy ("steps" or "epoch").
  eval_steps: 128                                  # Number of training steps between evaluations.
  logging_steps: 128                               # Interval (in steps) at which training metrics are logged.
  save_steps: 128                                  # Interval (in steps) at which model checkpoints are saved.
  per_device_train_batch_size: 8                   # Batch size per device during training.
  per_device_eval_batch_size: 8                    # Batch size per device for evaluation.
  gradient_accumulation_steps: 3                   # Number of steps to accumulate gradients before weight updates.
  learning_rate: 5e-05                             # Initial learning rate for the optimizer.
  load_best_model_at_end: True                     # Load the best model found during training at the end.
  metric_for_best_model: 'bleu'                    # Metric used to determine the best model.
  weight_decay: 0                                  # Weight decay factor (L2 regularization).
  adam_beta1: 0.9                                  # Beta1 parameter for the Adam optimizer.
  adam_beta2: 0.998                                # Beta2 parameter for the Adam optimizer.
  max_grad_norm: 0.0                               # Maximum gradient norm for clipping (0 means no clipping).
  num_train_epochs: 1                              # Number of full passes through the training dataset.
  max_steps: 500000                                # Maximum number of training steps (overrides num_train_epochs if set).
  lr_scheduler_type: "inverse_sqrt"                # Type of learning rate scheduler.
  warmup_steps: 8000                               # Number of warmup steps to gradually increase the learning rate.
  save_total_limit: 10                             # Maximum number of checkpoints to retain (older ones are deleted).
  seed: 3435                                       # Random seed for reproducibility.
  dataloader_drop_last: false                      # Drop the last incomplete batch in the dataloader.
  fp16: True                                       # Enable mixed-precision (FP16) training for faster computation.
  # See the list of allowed arguments in https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments

data:
  # Dataset and preprocessing settings:
  train_metadata_file: "/path/to/train/metadata_file.tsv"                                                                       # Path to the training metadata file (e.g., TSV format).
  validation_metadata_file: "/path/to/validation/metadata_file.tsv"                                                             # Path to the validation metadata file.
  test_metadata_file: "/path/to/test/metadata_file.tsv"                                                                         # Path to the test metadata file.
  shuffle: True                                                                                                                # Shuffle the dataset samples during loading.
  tokenizer_src_langs_path: "/examples/multimodal_translation/pose2text_translation/other/new_languages_how2sign.txt"          # Path to the file listing the source languages for the tokenizer.
  text_tokenizer_path: "<pretrained-tokenizer>"                                                                                # Path or identifier for the pretrained text tokenizer (e.g., "facebook/m2m100_418M").
  max_seq_length: 512                                                                                                          # Maximum number of tokens allowed in a sequence for tokenization/generation.
  max_frames: 300                                                                                                              # Maximum number of frames to consider in video samples; samples exceeding this are filtered.
