model:
  type: "multimodal_embedder" 
  feature_extractor_type: "<feature_extractor_type>"     # Type of the feature extractor to use (e.g., "clip" for CLIP).
  pretrained_feature_extractor: "<pretrained-clip-model>" # Pretrained weights/checkpoint for the feature extractor (e.g., "openai/clip-vit-base-patch32").
  vl_mapper_type: "linear"                               # Visual-language mapper type (e.g., "linear" or "adapter").
  vl_mapper_layer_norm_before: true                      # Apply Layer Normalization before the VL mapper.
  vl_mapper_dropout: 0.1                                 # Dropout probability for the VL mapper.
  backbone_name: "<pretrained-backbone-model>"          # Identifier for the pretrained backbone model (e.g., "m2m100").
  pretrained_backbone: "<pretrained-backbone-weights>"  # Pretrained weights for the backbone (e.g., "facebook/m2m100_418M").
  init_lang_abbr: avg                                    # Language abbreviation used to initialize new language embeddings.
  feature_extractor_cfg:                                 # Configuration for the feature extractor.
    feature_extractor_arguments: "<config-arguments>"   # Arguments to adjust the feature extractor's internal configuration.

training:
  run_name: "hebrew_multimodalhugs"                # The name or identifier of the model configuration.
  logging_dir: "/path/to/output/logs"              # Directory to store training logs.
  do_train: True                                   # Whether to run training.
  do_eval: True                                    # Whether to run evaluation on the validation set.
  predict_with_generate: true                      # Use generate to compute generative metrics.
  overwrite_output_dir: True                       # Do not overwrite the output directory if it exists.
  save_strategy: "steps"                           # Checkpoint save strategy during training.
  eval_strategy: "steps"                     # Evaluation strategy ("steps" or "epoch").
  eval_steps: 128                                  # Number of training steps between evaluations.
  logging_steps: 128                               # Interval (in steps) at which training metrics are logged.
  save_steps: 128                                  # Interval (in steps) at which model checkpoints are saved.
  per_device_train_batch_size: 8                   # Batch size per device during training.
  per_device_eval_batch_size: 8                    # Batch size per device for evaluation.
  gradient_accumulation_steps: 3                   # Number of steps to accumulate gradients before weight updates.
  learning_rate: 5e-05                             # Initial learning rate for the optimizer.
  load_best_model_at_end: True                     # Load the best model found during training at the end.
  metric_for_best_model: 'bleu'                    # Metric used to determine the best model.
  weight_decay: 0                                  # Weight decay factor (L2 regularization).
  adam_beta1: 0.9                                  # Beta1 parameter for the Adam optimizer.
  adam_beta2: 0.998                                # Beta2 parameter for the Adam optimizer.
  max_grad_norm: 0.0                               # Maximum gradient norm for clipping (0 means no clipping).
  num_train_epochs: 1                              # Number of full passes through the training dataset.
  max_steps: 500000                                # Maximum number of training steps (overrides num_train_epochs if set).
  lr_scheduler_type: "inverse_sqrt"                # Type of learning rate scheduler.
  warmup_steps: 8000                               # Number of warmup steps to gradually increase the learning rate.
  save_total_limit: 10                             # Maximum number of checkpoints to retain (older ones are deleted).
  seed: 3435                                       # Random seed for reproducibility.
  dataloader_drop_last: false                      # Drop the last incomplete batch in the dataloader.
  fp16: True                                       # Enable mixed-precision (FP16) training for faster computation.
  # See the list of allowed arguments in https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments

data:
  train_metadata_file: "/path/to/train/metadata_file.tsv"  # Path to the training metadata file.
  validation_metadata_file: "/path/to/validation/metadata_file.tsv"  # Path to the validation metadata file.
  test_metadata_file: "/path/to/test/metadata_file.tsv"    # Path to the test metadata file.
  font_path: "/examples/multimodal_translation/image2text_translation/other/Arial.ttf"  # Path to the font file used for generating images.
  as_numpy: false                                        # Whether to create images as numpy arrays (false means on-the-fly image generation).
  shuffle: True                                          # Shuffle dataset samples during loading.
  tokenizer_src_langs_path: "/examples/multimodal_translation/image2text_translation/other/new_languages_hebrew.txt"  # File listing source languages for the tokenizer.
  text_tokenizer_path: "<pretrained-tokenizer>"         # Pretrained text tokenizer identifier (e.g., "facebook/m2m100_418M").
  max_seq_length: 512                                    # Maximum token sequence length for text generation/tokenization.
  max_frames: 300                                       # Maximum number of frames to consider from the data.
  preprocess:                                          # Image preprocessing settings.
    width: 224                                         # Input image width in pixels.
    height: 224                                        # Input image height in pixels.
    channels: 3                                        # Number of image channels (e.g., 3 for RGB).
    dataset_mean: "<[mean_values]>"                     # Mean pixel values for image normalization (e.g., "[0.9819646859188279, 0.9819646859188279, 0.9819646859188279]").
    dataset_std: "<[std_values]>"                       # Standard deviation of pixel values for normalization (e.g., "[0.12833405937294548, 0.12833405937294548, 0.12833405937294548]").
