model:
  type: "multimodal_embedder" 
  feature_extractor_type: "<feature_extractor_type>"     # Type of the feature extractor to use (e.g., "clip" for CLIP).
  pretrained_feature_extractor: "<pretrained-clip-model>" # Pretrained weights/checkpoint for the feature extractor (e.g., "openai/clip-vit-base-patch32").
  multimodal_mapper_type: "linear"                               # Multimodal Mapper type (e.g., "linear" or "adapter").
  multimodal_mapper_layer_norm_before: true                      # Apply Layer Normalization before the Multimodal Mapper.
  multimodal_mapper_dropout: 0.1                                 # Dropout probability for the Multimodal Mapper.
  backbone_type: "<pretrained-backbone-model>"          # Identifier for the pretrained backbone model (e.g., "m2m100").
  pretrained_backbone: "<pretrained-backbone-weights>"  # Pretrained weights for the backbone (e.g., "facebook/m2m100_418M").
  feat_dim: 512                                           # Dimention of the Feature Extractor output. If features are extracted off-line, the dimentionality of features.
  feature_extractor_config:                                 # Configuration for the feature extractor.
    feature_extractor_arguments: "<config-arguments>"   # Arguments to adjust the feature extractor's internal configuration.

training:
  run_name: "hebrew_multimodalhugs"                # The name or identifier of the model configuration.
  output_dir: "/path/to/output/"                   # Directory to store training outputs.
  do_train: true                                   # Whether to run training.
  do_eval: true                                    # Whether to run evaluation on the validation set.
  eval_strategy: "steps"                           # Evaluation strategy ("steps" or "epoch").
  max_steps: 200000                                # Maximum number of training steps (overrides num_train_epochs if set).
  per_device_train_batch_size: 8                   # Batch size per device during training.
  gradient_accumulation_steps: 8                   # Number of steps to accumulate gradients before weight updates.
  per_device_eval_batch_size: 4                    # Batch size per device for evaluation.
  learning_rate: 5e-05                             # Initial learning rate for the optimizer.
  max_grad_norm: 0.0                               # Maximum gradient norm for clipping (0 means no clipping).
  lr_scheduler_type: "inverse_sqrt"                # Type of learning rate scheduler.
  warmup_steps: 20000                              # Number of warmup steps to gradually increase the learning rate.
  predict_with_generate: true                      # Use generate to compute generative metrics
  dataloader_num_workers: 4                        # Number of subprocesses to use for data loading; higher values speed up data loading but increase memory usage.
  dataloader_prefetch_factor: 2                    # Number of batches loaded in advance by each worker; total prefetched batches = num_workers * prefetch_factor.
  metric_name: bleu                                # Name of the metric to use (any metric supported by evaluate.load()). If you want to use multiple metrics, structure the variable like: metric_name: '<metric_name_1>,<metric_name_2>,...'
  metric_for_best_model: 'bleu'                    # Metric used to determine the best model.
  greater_is_better: true                          # Use in conjunction with load_best_model_at_end and metric_for_best_model to specify if better models should have a greater metric or not.
  load_best_model_at_end: true                     # Load the best model found during training at the end.
  seed: 3435                                       # Random seed for reproducibility.
  # See the list of allowed arguments in https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments

data:
  train_metadata_file: "/path/to/train/metadata_file.tsv"  # Path to the training metadata file.
  validation_metadata_file: "/path/to/validation/metadata_file.tsv"  # Path to the validation metadata file.
  test_metadata_file: "/path/to/test/metadata_file.tsv"    # Path to the test metadata file.
  new_vocabulary: "/examples/multimodal_translation/image2text_translation/other/new_languages_hebrew.txt"  # Path to the file containing the new tokens that will be added to the tokenizer.
  text_tokenizer_path: "<pretrained-tokenizer>"         # Pretrained text tokenizer identifier (e.g., "facebook/m2m100_418M").

processor:
  font_path: "/examples/multimodal_translation/image2text_translation/other/Arial.ttf"  # Path to the font file used for generating images.
  width: 224                                         # Input image width in pixels.
  height: 224                                        # Input image height in pixels.
  channels: 3                                        # Number of image channels (e.g., 3 for RGB).
  mean: "<[mean_values]>"                            # Mean pixel values for image normalization (e.g., "[0.9819646859188279, 0.9819646859188279, 0.9819646859188279]").
  std: "<[std_values]>"                              # Standard deviation of pixel values for normalization (e.g., "[0.12833405937294548, 0.12833405937294548, 0.12833405937294548]").