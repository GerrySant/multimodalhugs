# Multimodal Machine Translation

This framework focuses on translating sign language into written language using a multimodal approach. It leverages existing components from the [Hugging Face](https://huggingface.co/) ecosystem to facilitate efficient and robust model training. The multimodal setup includes handling both visual (sign language) and textual (spoken language) inputs for translation tasks.

Here are the current implemented examples:

- [SignWriting2Text](/examples/multimodal_translation/signwriting2text_translation/)
- [image2Text](examples/multimodal_translation/Image2text_translation)
- [Pose2Text](/examples/multimodal_translation/pose2text_translation/)
- Video2Text (Not available)



