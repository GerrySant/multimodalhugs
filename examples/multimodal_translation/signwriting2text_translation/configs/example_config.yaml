# Configuration file for training with the multimodalhugs framework.
# This file contains settings for the model, common experiment parameters,
# training hyperparameters, and dataset/data loading options.

data:
  train_metadata_file: "/path/to/train/metadata_file.tsv"        # Path to the training metadata file.
  validation_metadata_file: "/path/to/validation/metadata_file.tsv"  # Path to the validation metadata file.
  test_metadata_file: "/path/to/test/metadata_file.tsv"          # Path to the test metadata file.
  shuffle: True                                                # Shuffle dataset samples during loading.
  max_frames: 300                                             # Maximum number of frames to consider.

model:
  type: "multimodal_embedder" 
  feature_extractor_type: "<feature_extractor_type>"   # Type of feature extractor (e.g., "clip" for CLIP).
  pretrained_feature_extractor: "<pretrained-clip-model>"  # Pretrained weights for the feature extractor (e.g., "openai/clip-vit-base-patch32").
  multimodal_mapper_type: "linear"                            # Multimodal Mapper type (e.g., "linear" or "adapter").
  multimodal_mapper_layer_norm_before: true                   # Apply layer normalization before the Multimodal Mapper.
  multimodal_mapper_dropout: 0.1                              # Dropout probability for the Multimodal Mapper.
  backbone_type: "<pretrained-backbone-model>"         # Identifier for the pretrained backbone model (e.g., "m2m100").
  pretrained_backbone: "<pretrained-backbone-weights>"  # Pretrained weights for the backbone (e.g., "facebook/m2m100_418M").
  feat_dim: 512                                           # Dimention of the Feature Extractor output. If features are extracted off-line, the dimentionality of features.
  feature_extractor_config:                              # Configuration for modifying the feature extractor architecture.
    feature_extractor_arguments: "<config-arguments>"  # Arguments to adjust the internal configuration of the feature extractor.

processor:                                           # Image preprocessing settings.
  width: 224                                         # Input image width in pixels.
  height: 224                                        # Input image height in pixels.
  channels: 3                                        # Number of image channels (e.g., 3 for RGB).
  custom_preprocessor_path: openai/clip-vit-base-patch32 
  text_tokenizer_path: "<pretrained-tokenizer>"         # Pretrained text tokenizer identifier (e.g., "facebook/m2m100_418M").
  new_vocabulary: "/examples/multimodal_translation/image2text_translation/other/new_languages_hebrew.txt"  # Path to the file containing the new tokens that will be added to the tokenizer.

setup:
  output_dir: "</path/to/your/output_directory>"   # Base directory for saving setup artifacts
  do_dataset: true                                 # Whether to prepare the dataset during setup
  do_processor: true                               # Whether to set up the processor during setup
  do_model: true                                   # Whether to build the model during setup
  update_config: false                             # Whether to write created artifact paths back to the config file

training:
  run_name: "multimodal_embedder"                  # The name or identifier of the model configuration.
  logging_dir: "/path/to/output/logs"              # Directory to store training logs.
  do_train: True                                   # Whether to run training.
  do_eval: True                                    # Whether to run evaluation on the validation set.
  predict_with_generate: true                      # Use generate to compute generative metrics.
  save_strategy: "steps"                           # Checkpoint save strategy during training.
  eval_strategy: "steps"                     # Evaluation strategy ("steps" or "epoch").
  eval_steps: 128                                  # Number of training steps between evaluations.
  logging_steps: 128                               # Interval (in steps) at which training metrics are logged.
  save_steps: 128                                  # Interval (in steps) at which model checkpoints are saved.
  per_device_train_batch_size: 8                   # Batch size per device during training.
  per_device_eval_batch_size: 8                    # Batch size per device for evaluation.
  gradient_accumulation_steps: 3                   # Number of steps to accumulate gradients before weight updates.
  learning_rate: 5e-05                             # Initial learning rate for the optimizer.
  load_best_model_at_end: True                     # Load the best model found during training at the end.
  dataloader_num_workers: 4                        # Number of subprocesses to use for data loading; higher values speed up data loading but increase memory usage.
  dataloader_prefetch_factor: 2                    # Number of batches loaded in advance by each worker; total prefetched batches = num_workers * prefetch_factor.
  metric_name: bleu                                # Name of the metric to use (any metric supported by evaluate.load()). If you want to use multiple metrics, structure the variable like: metric_name: '<metric_name_1>,<metric_name_2>,...'
  metric_for_best_model: 'bleu'                    # Metric used to determine the best model.
  weight_decay: 0                                  # Weight decay factor (L2 regularization).
  adam_beta1: 0.9                                  # Beta1 parameter for the Adam optimizer.
  adam_beta2: 0.998                                # Beta2 parameter for the Adam optimizer.
  max_grad_norm: 0.0                               # Maximum gradient norm for clipping (0 means no clipping).
  num_train_epochs: 1                              # Number of full passes through the training dataset.
  max_steps: 500000                                # Maximum number of training steps (overrides num_train_epochs if set).
  lr_scheduler_type: "inverse_sqrt"                # Type of learning rate scheduler.
  warmup_steps: 8000                               # Number of warmup steps to gradually increase the learning rate.
  save_total_limit: 10                             # Maximum number of checkpoints to retain (older ones are deleted).
  seed: 3435                                       # Random seed for reproducibility.
  dataloader_drop_last: false                      # Drop the last incomplete batch in the dataloader.
  fp16: True                                       # Enable mixed-precision (FP16) training for faster computation.
  # See the list of allowed arguments in https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments