model:
  type: "multimodal_embedder" 
  feature_extractor_type: "<feature_extractor_type>"   # Type of feature extractor (e.g., "clip" for CLIP).
  pretrained_feature_extractor: "<pretrained-clip-model>"  # Pretrained weights for the feature extractor (e.g., "openai/clip-vit-base-patch32").
  multimodal_mapper_type: "linear"                            # Multimodal Mapper type (e.g., "linear" or "adapter").
  multimodal_mapper_layer_norm_before: true                   # Apply layer normalization before the Multimodal Mapper.
  multimodal_mapper_dropout: 0.1                              # Dropout probability for the Multimodal Mapper.
  backbone_type: "<pretrained-backbone-model>"         # Identifier for the pretrained backbone model (e.g., "m2m100").
  pretrained_backbone: "<pretrained-backbone-weights>"  # Pretrained weights for the backbone (e.g., "facebook/m2m100_418M").
  feat_dim: 512                                           # Dimention of the Feature Extractor output. If features are extracted off-line, the dimentionality of features.
  feature_extractor_config:                              # Configuration for modifying the feature extractor architecture.
    feature_extractor_arguments: "<config-arguments>"  # Arguments to adjust the internal configuration of the feature extractor.


training:
  run_name: "multimodal_embedder"                  # The name or identifier of the model configuration.
  logging_dir: "/path/to/output/logs"              # Directory to store training logs.
  do_train: True                                   # Whether to run training.
  do_eval: True                                    # Whether to run evaluation on the validation set.
  predict_with_generate: true                      # Use generate to compute generative metrics.
  overwrite_output_dir: True                       # Do not overwrite the output directory if it exists.
  save_strategy: "steps"                           # Checkpoint save strategy during training.
  eval_strategy: "steps"                     # Evaluation strategy ("steps" or "epoch").
  eval_steps: 128                                  # Number of training steps between evaluations.
  logging_steps: 128                               # Interval (in steps) at which training metrics are logged.
  save_steps: 128                                  # Interval (in steps) at which model checkpoints are saved.
  per_device_train_batch_size: 8                   # Batch size per device during training.
  per_device_eval_batch_size: 8                    # Batch size per device for evaluation.
  gradient_accumulation_steps: 3                   # Number of steps to accumulate gradients before weight updates.
  learning_rate: 5e-05                             # Initial learning rate for the optimizer.
  load_best_model_at_end: True                     # Load the best model found during training at the end.
  metric_for_best_model: 'bleu'                    # Metric used to determine the best model.
  weight_decay: 0                                  # Weight decay factor (L2 regularization).
  adam_beta1: 0.9                                  # Beta1 parameter for the Adam optimizer.
  adam_beta2: 0.998                                # Beta2 parameter for the Adam optimizer.
  max_grad_norm: 0.0                               # Maximum gradient norm for clipping (0 means no clipping).
  num_train_epochs: 1                              # Number of full passes through the training dataset.
  max_steps: 500000                                # Maximum number of training steps (overrides num_train_epochs if set).
  lr_scheduler_type: "inverse_sqrt"                # Type of learning rate scheduler.
  warmup_steps: 8000                               # Number of warmup steps to gradually increase the learning rate.
  save_total_limit: 10                             # Maximum number of checkpoints to retain (older ones are deleted).
  seed: 3435                                       # Random seed for reproducibility.
  dataloader_drop_last: false                      # Drop the last incomplete batch in the dataloader.
  fp16: True                                       # Enable mixed-precision (FP16) training for faster computation.
  # See the list of allowed arguments in https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments

data:
  train_metadata_file: "/path/to/train/metadata_file.tsv"        # Path to the training metadata file.
  validation_metadata_file: "/path/to/validation/metadata_file.tsv"  # Path to the validation metadata file.
  test_metadata_file: "/path/to/test/metadata_file.tsv"          # Path to the test metadata file.
  shuffle: True                                                # Shuffle dataset samples during loading.
  new_vocabulary: "/examples/multimodal_translation/signwritting2text_translation/other/new_languages_sign_bank_plus.txt"  # Path to the file containing the new tokens that will be added to the tokenizer.
  text_tokenizer_path: "<pretrained-tokenizer>"                 # Pretrained text tokenizer identifier (e.g., "facebook/m2m100_418M").
  max_frames: 300                                             # Maximum number of frames to consider.
  preprocess:                                                 # Image preprocessing configuration.
    width: 224                                               # Target image width in pixels.
    height: 224                                              # Target image height in pixels.
    channels: 3                                              # Number of image channels (e.g., 3 for RGB).
    dataset_mean: "<[mean_values]>"                            # Mean pixel values for normalization (e.g., "[0.9819646859188279, 0.9819646859188279, 0.9819646859188279]").
    dataset_std: "<[std_values]>"                              # Standard deviation for normalization (e.g., "[0.12833405937294548, 0.12833405937294548, 0.12833405937294548]").
